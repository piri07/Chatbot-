# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/176g3_01xU5kScO-lCCG-uK9lLiu2ZP6D
"""

import nltk
nltk.download('punkt') #tokenize sentences to individual words

from nltk.stem.lancaster import LancasterStemmer #algo for stemming
stemmer = LancasterStemmer() #storing object lancaster stemmer
#reduces the variation of words to root words.

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet') 
lemmatizer = WordNetLemmatizer()

!pip install --upgrade tensorflow

import tensorflow as tf
import numpy as np
import random #shuffling the feature
import json

!pip install tflearn

import tflearn #high level abstraction for tensorflow , used for creating DNN

from google.colab import files
files.upload()

with open('intents.json') as jsons: #reading the files 
  intents = json.load(jsons)
intents

words=[]
classes = []
ignore = ['?'] #for exculding punctuation mark.
documents = []
for intent in intents['intents']:
  for patterns in intent['patterns']:
    w=nltk.word_tokenize(patterns)
    words.extend(w)
    documents.append((w,intent['tag']))
    if intent['tag'] not in classes:
      classes.append(intent['tag'])
print(words)
print(classes)
print(documents)

words = [stemmer.stem(w.lower()) for w in words if w not in ignore]
words = sorted(list(set(words)))
print(words)

#words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore]
#words = sorted(list(set(words)))
#print(words)

#both stemming and lematization gives same result

classes = sorted(list(set(classes)))
print(classes)

training = []
output=[]
output_empty = [0]*len(classes) #create empty array for output

for doc in documents :
  bag=[]
  pattern_words = doc[0]#[(sentens,tag)] 
  pattern_words = [stemmer.stem(w.lower()) for w in pattern_words] #stem each word in the patterns file
  for w in words:
    if w in pattern_words:
      bag.append(1)
    else:
      bag.append(0)

  output_row = list(output_empty)
  #print(output_row)
  output_row[classes.index(doc[1])]=1 #mark the index of tag as 1 to which the sentence belong. ex hello how are you to greetings.
  #print(output_row)
  training.append([bag,output_row]) #create training dataset
print(training)
print(output_row)

random.shuffle(training)
training=np.array(training)
#print(training)

train_x = list(training[:,0]) #features
train_y = list(training[:,1]) #labels
#print(len(train_x[0]))

tf.compat.v1.reset_default_graph()
#building neural network
net = tflearn.input_data(shape=[None,len(train_x[0])])
net = tflearn.fully_connected(net, 10)
net = tflearn.fully_connected(net, 10)
net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')
net = tflearn.regression(net)

model = tflearn.DNN(net,tensorboard_dir='tflearn_logs')
model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)
model.save('model.tflearn')

import pickle
pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( "training_data", "wb" ) )

data = pickle.load(open('training_data','rb'))
words=data['words']
classes = data['classes']
train_x = data['train_x']
train_y = data['train_y']

with open('intents.json') as json_data:
    intents = json.load(json_data)
print(intents)

model.load('./model.tflearn') #load the saved model

def clean_sentences(sentence):
  sentence_words = nltk.word_tokenize(sentence)
  sentence_words = [stemmer.stem(words.lower()) for words in sentence_words]

  return sentence_words

# returning bag of words array: 0 or 1 for each word in the bag that exists in the sentence
def bow(sentence,words,show_details=False):
  sentence_words = clean_sentences(sentence)
  bag=[0]*len(words)
  for s in sentence_words:
    for i,w in enumerate(words):
      if w==s:
        bag[i]=1
        if show_details:
          print("found in bag: %s" %w)
  return np.array(bag)

threshold = 0.30
def classify(sentence):
  result = model.predict([bow(sentence,words)])[0]
  result = [[i,r] for i,r in enumerate(result) if r>threshold]
  result.sort(key = lambda x:x[1],reverse=True)
  final_list=[]
  for r in result:
    final_list.append((classes[r[0]],r[1]))
  return final_list

def response(sentence,userID='123',show_details='False'):
  results = classify(sentence)
  if results:
    while results:
      for i in intents['intents']:
        if i['tag']==results[0][0]:
          return print(random.choice(i['responses']))
      results.pop(0)

classify('What are you hours of operation?')

response('What is menu for today?')

response("Chalo bhai bye bye")